{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read App Events\n",
      "# Read Events\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "# User-Item-Feature\n",
      "# Feature Selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3-4.1.1\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ..., 0 0 0] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Num of Features:  4823\n",
      "[0]\ttrain-mlogloss:2.4114\teval-mlogloss:2.42106\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.37642\teval-mlogloss:2.39219\n",
      "[2]\ttrain-mlogloss:2.35135\teval-mlogloss:2.37236\n",
      "[3]\ttrain-mlogloss:2.33158\teval-mlogloss:2.35727\n",
      "[4]\ttrain-mlogloss:2.31525\teval-mlogloss:2.34518\n",
      "[5]\ttrain-mlogloss:2.30141\teval-mlogloss:2.33525\n",
      "[6]\ttrain-mlogloss:2.28949\teval-mlogloss:2.32694\n",
      "[7]\ttrain-mlogloss:2.27909\teval-mlogloss:2.31991\n",
      "[8]\ttrain-mlogloss:2.26994\teval-mlogloss:2.31392\n",
      "[9]\ttrain-mlogloss:2.26184\teval-mlogloss:2.30878\n",
      "[10]\ttrain-mlogloss:2.25461\teval-mlogloss:2.30434\n",
      "[11]\ttrain-mlogloss:2.24813\teval-mlogloss:2.30051\n",
      "[12]\ttrain-mlogloss:2.24229\teval-mlogloss:2.29718\n",
      "[13]\ttrain-mlogloss:2.23701\teval-mlogloss:2.29427\n",
      "[14]\ttrain-mlogloss:2.23221\teval-mlogloss:2.29173\n",
      "[15]\ttrain-mlogloss:2.22784\teval-mlogloss:2.28951\n",
      "[16]\ttrain-mlogloss:2.22384\teval-mlogloss:2.28756\n",
      "[17]\ttrain-mlogloss:2.22018\teval-mlogloss:2.28586\n",
      "[18]\ttrain-mlogloss:2.21682\teval-mlogloss:2.28436\n",
      "[19]\ttrain-mlogloss:2.21372\teval-mlogloss:2.28304\n",
      "[20]\ttrain-mlogloss:2.21085\teval-mlogloss:2.28188\n",
      "[21]\ttrain-mlogloss:2.2082\teval-mlogloss:2.28087\n",
      "[22]\ttrain-mlogloss:2.20574\teval-mlogloss:2.27998\n",
      "[23]\ttrain-mlogloss:2.20345\teval-mlogloss:2.2792\n",
      "[24]\ttrain-mlogloss:2.20133\teval-mlogloss:2.27851\n",
      "[25]\ttrain-mlogloss:2.19934\teval-mlogloss:2.27791\n",
      "[26]\ttrain-mlogloss:2.19749\teval-mlogloss:2.27739\n",
      "[27]\ttrain-mlogloss:2.19575\teval-mlogloss:2.27693\n",
      "[28]\ttrain-mlogloss:2.19412\teval-mlogloss:2.27653\n",
      "[29]\ttrain-mlogloss:2.1926\teval-mlogloss:2.27619\n",
      "[30]\ttrain-mlogloss:2.19116\teval-mlogloss:2.2759\n",
      "[31]\ttrain-mlogloss:2.18981\teval-mlogloss:2.27564\n",
      "[32]\ttrain-mlogloss:2.18854\teval-mlogloss:2.27543\n",
      "[33]\ttrain-mlogloss:2.18734\teval-mlogloss:2.27525\n",
      "[34]\ttrain-mlogloss:2.18621\teval-mlogloss:2.2751\n",
      "[35]\ttrain-mlogloss:2.18513\teval-mlogloss:2.27497\n",
      "[36]\ttrain-mlogloss:2.18412\teval-mlogloss:2.27487\n",
      "[37]\ttrain-mlogloss:2.18316\teval-mlogloss:2.27479\n",
      "[38]\ttrain-mlogloss:2.18225\teval-mlogloss:2.27473\n",
      "[39]\ttrain-mlogloss:2.18138\teval-mlogloss:2.27469\n",
      "# Train\n"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "# Based on yibo's R script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "#from sklearn.feature_extraction import FeatureHasher\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "#from sklearn.decomposition import TruncatedSVD, SparsePCA\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#from sklearn.metrics import log_loss\n",
    "\n",
    "# Create bag-of-apps in character string format\n",
    "# first by event\n",
    "# then merge to generate larger bags by device\n",
    "\n",
    "##################\n",
    "#   App Events\n",
    "##################\n",
    "print(\"# Read App Events\")\n",
    "app_ev = pd.read_csv(\"F:kaggle_data/talking_data_mobile/app_events.csv\", usecols=['event_id', 'app_id'])\n",
    "# remove duplicates(app_id)\n",
    "app_ev = app_ev.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "\n",
    "##################\n",
    "#     Events\n",
    "##################\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(\"F:kaggle_data/talking_data_mobile/events.csv\", usecols=['event_id','device_id'])\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_ev)\n",
    "\n",
    "events = events.dropna()\n",
    "\n",
    "del app_ev\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"F:kaggle_data/talking_data_mobile/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_train.csv\", usecols=['device_id','group'])\n",
    "\n",
    "test = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_test.csv\")\n",
    "                  \n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "\n",
    "del Df\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3), axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.07,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 40, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)\n",
    "\n",
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 40, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('fine_tune.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_id\n",
       "2     app_id:7460082553072507347 app_id:-17588575798...\n",
       "6     app_id:-5839858269967688123 app_id:-1633912854...\n",
       "7     app_id:-5839858269967688123 app_id:-5408952623...\n",
       "9     app_id:-5839858269967688123 app_id:74600825530...\n",
       "16    app_id:7460082553072507347 app_id:353941197706...\n",
       "Name: app_id, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_ev = pd.read_pickle('F:/app_ev')\n",
    "app_ev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add time group (零次，少次，多次)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read Events\n",
      "(3252950, 4)\n",
      "after dropna: (1488096, 4)\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "add time group FLS.shape: (2803279, 2)\n",
      "# User-Item-Feature\n",
      "device_ids: (189076,)\n",
      "feature_cs: (20970,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: str() > int()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-ff87f670a475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;31m##################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m \u001b[0mtrain_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"device_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[0mtrain_sp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3-4.1.1\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0m_check_numpy_unicode_bug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y contains new labels: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3-4.1.1\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36mintersect1d\u001b[1;34m(ar1, ar2, assume_unique)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mar2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m     \u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unorderable types: str() > int()"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# coding=utf8\n",
    "# Based on yibo's R script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "#from sklearn.feature_extraction import FeatureHasher\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "#from sklearn.decomposition import TruncatedSVD, SparsePCA\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#from sklearn.metrics import log_loss\n",
    "\n",
    "# Create bag-of-apps in character string format\n",
    "# first by event\n",
    "# then merge to generate larger bags by device\n",
    "\n",
    "##################\n",
    "#   App Events\n",
    "##################\n",
    "#print(\"# Read App Events\")\n",
    "#app_ev = pd.read_csv(\"F:kaggle_data/talking_data_mobile/app_events.csv\", usecols=['event_id', 'app_id'])\n",
    "# remove duplicates(app_id)\n",
    "#app_ev = app_ev.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "#    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "\n",
    "#app_ev.to_pickle('F:/app_ev')\n",
    "app_ev = pd.read_pickle('F:/app_ev')\n",
    "##################\n",
    "#     Events\n",
    "##################\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(\"F:kaggle_data/talking_data_mobile/events.csv\", usecols=['event_id','device_id','timestamp'])\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_ev)\n",
    "\n",
    "del app_ev\n",
    "\n",
    "print(events.shape)\n",
    "events = events.dropna()\n",
    "print('after dropna:' , events.shape)\n",
    "\n",
    "#extract time feature\n",
    "events['hour'] = pd.to_datetime(events.timestamp).dt.hour\n",
    "\n",
    "#when the event happen in the events dataset\n",
    "#%matplotlib inline\n",
    "#events.hour.plot(kind='hist',bins=24)\n",
    "\n",
    "# 1 to 4 clock in the morning \n",
    "events.loc[events.hour <= 4,'count'] = 1.0\n",
    "events.loc[events.hour > 4,'count'] = 0.0\n",
    "\n",
    "# how many times a person play in the morning \n",
    "counts = events.groupby('device_id')['count'].sum()\n",
    "#group them\n",
    "ind1 = counts < 4\n",
    "ind2 = (counts >3) & (counts <10)\n",
    "ind3 = counts >9\n",
    "counts[ind1] = '少次'\n",
    "counts[ind2] = '中次'\n",
    "counts[ind3] = '多次'\n",
    "\n",
    "del ind1, ind2, ind3\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"F:kaggle_data/talking_data_mobile/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_train.csv\", usecols=['device_id','group'])\n",
    "\n",
    "test = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_test.csv\")\n",
    "                  \n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "f4 = counts.reset_index()               # time group\n",
    "\n",
    "del Df\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "f4.columns.values[1] = 'feature'\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, f4), axis=0, ignore_index=True)\n",
    "\n",
    "print('add time group FLS.shape:' , FLS.shape)\n",
    "\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "print('device_ids:', device_ids.shape)\n",
    "print('feature_cs:', feature_cs.shape)\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "print('add time train_sp:', train_sp.shape)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "print('after select time train_sp:', train_sp.shape)\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.05,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 35, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add time train_sp: (74645, 20970)\n",
      "# Feature Selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3-4.1.1\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ..., 0 0 0] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after select time train_sp: (74645, 4823)\n",
      "# Num of Features:  4823\n",
      "[0]\ttrain-mlogloss:2.42447\teval-mlogloss:2.43216\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.39394\teval-mlogloss:2.40653\n",
      "[2]\ttrain-mlogloss:2.37181\teval-mlogloss:2.38859\n",
      "[3]\ttrain-mlogloss:2.35408\teval-mlogloss:2.3746\n",
      "[4]\ttrain-mlogloss:2.33916\teval-mlogloss:2.36309\n",
      "[5]\ttrain-mlogloss:2.32625\teval-mlogloss:2.35336\n",
      "[6]\ttrain-mlogloss:2.31491\teval-mlogloss:2.34499\n",
      "[7]\ttrain-mlogloss:2.30482\teval-mlogloss:2.33771\n",
      "[8]\ttrain-mlogloss:2.29577\teval-mlogloss:2.33131\n",
      "[9]\ttrain-mlogloss:2.28761\teval-mlogloss:2.32566\n",
      "[10]\ttrain-mlogloss:2.2802\teval-mlogloss:2.32064\n",
      "[11]\ttrain-mlogloss:2.27344\teval-mlogloss:2.31617\n",
      "[12]\ttrain-mlogloss:2.26725\teval-mlogloss:2.31217\n",
      "[13]\ttrain-mlogloss:2.26157\teval-mlogloss:2.30859\n",
      "[14]\ttrain-mlogloss:2.25633\teval-mlogloss:2.30535\n",
      "[15]\ttrain-mlogloss:2.25149\teval-mlogloss:2.30244\n",
      "[16]\ttrain-mlogloss:2.247\teval-mlogloss:2.29981\n",
      "[17]\ttrain-mlogloss:2.24284\teval-mlogloss:2.29742\n",
      "[18]\ttrain-mlogloss:2.23896\teval-mlogloss:2.29526\n",
      "[19]\ttrain-mlogloss:2.23534\teval-mlogloss:2.2933\n",
      "[20]\ttrain-mlogloss:2.23197\teval-mlogloss:2.29152\n",
      "[21]\ttrain-mlogloss:2.2288\teval-mlogloss:2.2899\n",
      "[22]\ttrain-mlogloss:2.22584\teval-mlogloss:2.28842\n",
      "[23]\ttrain-mlogloss:2.22306\teval-mlogloss:2.28708\n",
      "[24]\ttrain-mlogloss:2.22044\teval-mlogloss:2.28585\n",
      "[25]\ttrain-mlogloss:2.21798\teval-mlogloss:2.28473\n",
      "[26]\ttrain-mlogloss:2.21566\teval-mlogloss:2.28372\n",
      "[27]\ttrain-mlogloss:2.21347\teval-mlogloss:2.28279\n",
      "[28]\ttrain-mlogloss:2.2114\teval-mlogloss:2.28194\n",
      "[29]\ttrain-mlogloss:2.20944\teval-mlogloss:2.28117\n",
      "[30]\ttrain-mlogloss:2.20759\teval-mlogloss:2.28046\n",
      "[31]\ttrain-mlogloss:2.20583\teval-mlogloss:2.27982\n",
      "[32]\ttrain-mlogloss:2.20416\teval-mlogloss:2.27924\n",
      "[33]\ttrain-mlogloss:2.20257\teval-mlogloss:2.2787\n",
      "[34]\ttrain-mlogloss:2.20107\teval-mlogloss:2.27822\n"
     ]
    }
   ],
   "source": [
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "print('add time train_sp:', train_sp.shape)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "print('after select time train_sp:', train_sp.shape)\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.05,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 35, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train\n"
     ]
    }
   ],
   "source": [
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 30, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('fine_tune.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 每次一个feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read Events\n",
      "(3252950, 4)\n",
      "after dropna: (1488096, 4)\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "add time group FLS.shape: (2803279, 2)\n",
      "# User-Item-Feature\n",
      "device_ids: (189076,)\n",
      "feature_cs: (21084,)\n",
      "add time train_sp: (74645, 21084)\n",
      "# Feature Selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3-4.1.1\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ..., 0 0 0] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after select time train_sp: (74645, 4850)\n",
      "# Num of Features:  4850\n",
      "[0]\ttrain-mlogloss:2.42432\teval-mlogloss:2.43211\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.39381\teval-mlogloss:2.40653\n",
      "[2]\ttrain-mlogloss:2.37169\teval-mlogloss:2.38862\n",
      "[3]\ttrain-mlogloss:2.35395\teval-mlogloss:2.37463\n",
      "[4]\ttrain-mlogloss:2.33901\teval-mlogloss:2.36313\n",
      "[5]\ttrain-mlogloss:2.32609\teval-mlogloss:2.35339\n",
      "[6]\ttrain-mlogloss:2.31473\teval-mlogloss:2.34502\n",
      "[7]\ttrain-mlogloss:2.30463\teval-mlogloss:2.33774\n",
      "[8]\ttrain-mlogloss:2.29557\teval-mlogloss:2.33135\n",
      "[9]\ttrain-mlogloss:2.28739\teval-mlogloss:2.32571\n",
      "[10]\ttrain-mlogloss:2.27996\teval-mlogloss:2.32069\n",
      "[11]\ttrain-mlogloss:2.27319\teval-mlogloss:2.31621\n",
      "[12]\ttrain-mlogloss:2.26699\teval-mlogloss:2.31221\n",
      "[13]\ttrain-mlogloss:2.26129\teval-mlogloss:2.30862\n",
      "[14]\ttrain-mlogloss:2.25604\teval-mlogloss:2.30538\n",
      "[15]\ttrain-mlogloss:2.25119\teval-mlogloss:2.30247\n",
      "[16]\ttrain-mlogloss:2.24669\teval-mlogloss:2.29983\n",
      "[17]\ttrain-mlogloss:2.24251\teval-mlogloss:2.29744\n",
      "[18]\ttrain-mlogloss:2.23862\teval-mlogloss:2.29528\n",
      "[19]\ttrain-mlogloss:2.235\teval-mlogloss:2.29331\n",
      "[20]\ttrain-mlogloss:2.23161\teval-mlogloss:2.29153\n",
      "[21]\ttrain-mlogloss:2.22844\teval-mlogloss:2.2899\n",
      "[22]\ttrain-mlogloss:2.22546\teval-mlogloss:2.28842\n",
      "[23]\ttrain-mlogloss:2.22267\teval-mlogloss:2.28708\n",
      "[24]\ttrain-mlogloss:2.22005\teval-mlogloss:2.28585\n",
      "[25]\ttrain-mlogloss:2.21758\teval-mlogloss:2.28473\n",
      "[26]\ttrain-mlogloss:2.21525\teval-mlogloss:2.28371\n",
      "[27]\ttrain-mlogloss:2.21305\teval-mlogloss:2.28277\n",
      "[28]\ttrain-mlogloss:2.21098\teval-mlogloss:2.28192\n",
      "[29]\ttrain-mlogloss:2.20901\teval-mlogloss:2.28115\n",
      "[30]\ttrain-mlogloss:2.20715\teval-mlogloss:2.28044\n",
      "[31]\ttrain-mlogloss:2.20539\teval-mlogloss:2.2798\n",
      "[32]\ttrain-mlogloss:2.20371\teval-mlogloss:2.27921\n",
      "[33]\ttrain-mlogloss:2.20212\teval-mlogloss:2.27867\n",
      "[34]\ttrain-mlogloss:2.20061\teval-mlogloss:2.27818\n"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "# Based on yibo's R script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "#from sklearn.feature_extraction import FeatureHasher\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "#from sklearn.decomposition import TruncatedSVD, SparsePCA\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#from sklearn.metrics import log_loss\n",
    "\n",
    "# Create bag-of-apps in character string format\n",
    "# first by event\n",
    "# then merge to generate larger bags by device\n",
    "\n",
    "##################\n",
    "#   App Events\n",
    "##################\n",
    "#print(\"# Read App Events\")\n",
    "#app_ev = pd.read_csv(\"F:kaggle_data/talking_data_mobile/app_events.csv\", usecols=['event_id', 'app_id'])\n",
    "# remove duplicates(app_id)\n",
    "#app_ev = app_ev.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "#    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "\n",
    "#app_ev.to_pickle('F:/app_ev')\n",
    "app_ev = pd.read_pickle('F:/app_ev')\n",
    "##################\n",
    "#     Events\n",
    "##################\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(\"F:kaggle_data/talking_data_mobile/events.csv\", usecols=['event_id','device_id','timestamp'])\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_ev)\n",
    "\n",
    "del app_ev\n",
    "\n",
    "print(events.shape)\n",
    "events = events.dropna()\n",
    "print('after dropna:' , events.shape)\n",
    "\n",
    "#extract time feature\n",
    "events['hour'] = pd.to_datetime(events.timestamp).dt.hour\n",
    "\n",
    "#when the event happen in the events dataset\n",
    "#%matplotlib inline\n",
    "#events.hour.plot(kind='hist',bins=24)\n",
    "\n",
    "# 1 to 4 clock in the morning \n",
    "events.loc[events.hour <= 4,'count'] = 1.0\n",
    "events.loc[events.hour > 4,'count'] = 0.0\n",
    "\n",
    "# how many times a person play in the morning \n",
    "counts = events.groupby('device_id')['count'].sum()\n",
    "#group them\n",
    "counts = counts.reset_index()\n",
    "counts['count'] = counts['count'].astype(np.str) + '次'\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"F:kaggle_data/talking_data_mobile/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_train.csv\", usecols=['device_id','group'])\n",
    "\n",
    "test = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_test.csv\")\n",
    "                  \n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "         # time group\n",
    "\n",
    "del Df\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "counts.columns.values[1] = 'feature'\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, counts), axis=0, ignore_index=True)\n",
    "\n",
    "print('add time group FLS.shape:' , FLS.shape)\n",
    "\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "print('device_ids:', device_ids.shape)\n",
    "print('feature_cs:', feature_cs.shape)\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "print('add time train_sp:', train_sp.shape)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "print('after select time train_sp:', train_sp.shape)\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.05,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 35, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.4208\teval-mlogloss:2.42912\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.38912\teval-mlogloss:2.4027\n",
      "[2]\ttrain-mlogloss:2.36616\teval-mlogloss:2.38423\n",
      "[3]\ttrain-mlogloss:2.34781\teval-mlogloss:2.36989\n",
      "[4]\ttrain-mlogloss:2.33243\teval-mlogloss:2.35818\n",
      "[5]\ttrain-mlogloss:2.3192\teval-mlogloss:2.34833\n",
      "[6]\ttrain-mlogloss:2.30761\teval-mlogloss:2.33993\n",
      "[7]\ttrain-mlogloss:2.29737\teval-mlogloss:2.33266\n",
      "[8]\ttrain-mlogloss:2.28822\teval-mlogloss:2.32633\n",
      "[9]\ttrain-mlogloss:2.28001\teval-mlogloss:2.32077\n",
      "[10]\ttrain-mlogloss:2.27258\teval-mlogloss:2.31587\n",
      "[11]\ttrain-mlogloss:2.26584\teval-mlogloss:2.31154\n",
      "[12]\ttrain-mlogloss:2.25969\teval-mlogloss:2.30768\n",
      "[13]\ttrain-mlogloss:2.25407\teval-mlogloss:2.30424\n",
      "[14]\ttrain-mlogloss:2.24891\teval-mlogloss:2.30117\n",
      "[15]\ttrain-mlogloss:2.24415\teval-mlogloss:2.29842\n",
      "[16]\ttrain-mlogloss:2.23976\teval-mlogloss:2.29596\n",
      "[17]\ttrain-mlogloss:2.2357\teval-mlogloss:2.29374\n",
      "[18]\ttrain-mlogloss:2.23193\teval-mlogloss:2.29175\n",
      "[19]\ttrain-mlogloss:2.22843\teval-mlogloss:2.28995\n",
      "[20]\ttrain-mlogloss:2.22517\teval-mlogloss:2.28833\n",
      "[21]\ttrain-mlogloss:2.22212\teval-mlogloss:2.28687\n",
      "[22]\ttrain-mlogloss:2.21928\teval-mlogloss:2.28555\n",
      "[23]\ttrain-mlogloss:2.21662\teval-mlogloss:2.28436\n",
      "[24]\ttrain-mlogloss:2.21412\teval-mlogloss:2.28328\n",
      "[25]\ttrain-mlogloss:2.21177\teval-mlogloss:2.2823\n",
      "[26]\ttrain-mlogloss:2.20957\teval-mlogloss:2.28142\n",
      "[27]\ttrain-mlogloss:2.20749\teval-mlogloss:2.28062\n",
      "[28]\ttrain-mlogloss:2.20554\teval-mlogloss:2.27991\n",
      "[29]\ttrain-mlogloss:2.20369\teval-mlogloss:2.27926\n",
      "[30]\ttrain-mlogloss:2.20194\teval-mlogloss:2.27867\n",
      "[31]\ttrain-mlogloss:2.20029\teval-mlogloss:2.27814\n",
      "[32]\ttrain-mlogloss:2.19873\teval-mlogloss:2.27766\n",
      "[33]\ttrain-mlogloss:2.19725\teval-mlogloss:2.27723\n",
      "[34]\ttrain-mlogloss:2.19584\teval-mlogloss:2.27685\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.055,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 35, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train\n"
     ]
    }
   ],
   "source": [
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 35, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('fine_tune.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.41116\teval-mlogloss:2.42091\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.37617\teval-mlogloss:2.3921\n",
      "[2]\ttrain-mlogloss:2.35108\teval-mlogloss:2.37231\n",
      "[3]\ttrain-mlogloss:2.33127\teval-mlogloss:2.35722\n",
      "[4]\ttrain-mlogloss:2.31491\teval-mlogloss:2.34513\n",
      "[5]\ttrain-mlogloss:2.30103\teval-mlogloss:2.33518\n",
      "[6]\ttrain-mlogloss:2.28907\teval-mlogloss:2.32687\n",
      "[7]\ttrain-mlogloss:2.27863\teval-mlogloss:2.31982\n",
      "[8]\ttrain-mlogloss:2.26945\teval-mlogloss:2.31382\n",
      "[9]\ttrain-mlogloss:2.26132\teval-mlogloss:2.30866\n",
      "[10]\ttrain-mlogloss:2.25406\teval-mlogloss:2.30422\n",
      "[11]\ttrain-mlogloss:2.24755\teval-mlogloss:2.30036\n",
      "[12]\ttrain-mlogloss:2.24168\teval-mlogloss:2.29702\n",
      "[13]\ttrain-mlogloss:2.23637\teval-mlogloss:2.29409\n",
      "[14]\ttrain-mlogloss:2.23155\teval-mlogloss:2.29154\n",
      "[15]\ttrain-mlogloss:2.22716\teval-mlogloss:2.28931\n",
      "[16]\ttrain-mlogloss:2.22314\teval-mlogloss:2.28735\n",
      "[17]\ttrain-mlogloss:2.21946\teval-mlogloss:2.28563\n",
      "[18]\ttrain-mlogloss:2.21608\teval-mlogloss:2.28412\n",
      "[19]\ttrain-mlogloss:2.21296\teval-mlogloss:2.28279\n",
      "[20]\ttrain-mlogloss:2.21007\teval-mlogloss:2.28163\n",
      "[21]\ttrain-mlogloss:2.20741\teval-mlogloss:2.28059\n",
      "[22]\ttrain-mlogloss:2.20493\teval-mlogloss:2.27969\n",
      "[23]\ttrain-mlogloss:2.20263\teval-mlogloss:2.2789\n",
      "[24]\ttrain-mlogloss:2.20049\teval-mlogloss:2.27821\n",
      "[25]\ttrain-mlogloss:2.1985\teval-mlogloss:2.2776\n",
      "[26]\ttrain-mlogloss:2.19663\teval-mlogloss:2.27707\n",
      "[27]\ttrain-mlogloss:2.19489\teval-mlogloss:2.27661\n",
      "[28]\ttrain-mlogloss:2.19325\teval-mlogloss:2.27621\n",
      "[29]\ttrain-mlogloss:2.19171\teval-mlogloss:2.27586\n",
      "[30]\ttrain-mlogloss:2.19027\teval-mlogloss:2.27556\n",
      "[31]\ttrain-mlogloss:2.18892\teval-mlogloss:2.2753\n",
      "[32]\ttrain-mlogloss:2.18764\teval-mlogloss:2.27508\n",
      "[33]\ttrain-mlogloss:2.18643\teval-mlogloss:2.27489\n",
      "[34]\ttrain-mlogloss:2.18529\teval-mlogloss:2.27473\n",
      "[35]\ttrain-mlogloss:2.18421\teval-mlogloss:2.27461\n",
      "[36]\ttrain-mlogloss:2.18319\teval-mlogloss:2.2745\n",
      "[37]\ttrain-mlogloss:2.18223\teval-mlogloss:2.27442\n",
      "[38]\ttrain-mlogloss:2.18131\teval-mlogloss:2.27435\n",
      "[39]\ttrain-mlogloss:2.18045\teval-mlogloss:2.27431\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.07,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 40, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train\n"
     ]
    }
   ],
   "source": [
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 35, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('fine_tune.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without add time group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read Events\n",
      "(3252950, 4)\n",
      "(1488096, 4)\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      " not add time group FLS.shape: (2742457, 2)\n",
      "# User-Item-Feature\n",
      "device_ids: (189076,)\n",
      "feature_cs: (20967,)\n",
      "not add time train_sp: (74645, 20967)\n",
      "# Feature Selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3-4.1.1\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ..., 0 0 0] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after select time train_sp: (74645, 4823)\n",
      "# Num of Features:  4823\n",
      "[0]\ttrain-mlogloss:2.41138\teval-mlogloss:2.42104\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.37627\teval-mlogloss:2.39207\n",
      "[2]\ttrain-mlogloss:2.3512\teval-mlogloss:2.37224\n",
      "[3]\ttrain-mlogloss:2.33145\teval-mlogloss:2.35717\n",
      "[4]\ttrain-mlogloss:2.31514\teval-mlogloss:2.3451\n",
      "[5]\ttrain-mlogloss:2.30131\teval-mlogloss:2.33518\n",
      "[6]\ttrain-mlogloss:2.2894\teval-mlogloss:2.32688\n",
      "[7]\ttrain-mlogloss:2.27901\teval-mlogloss:2.31986\n",
      "[8]\ttrain-mlogloss:2.26988\teval-mlogloss:2.31388\n",
      "[9]\ttrain-mlogloss:2.26178\teval-mlogloss:2.30874\n",
      "[10]\ttrain-mlogloss:2.25456\teval-mlogloss:2.30432\n",
      "[11]\ttrain-mlogloss:2.24808\teval-mlogloss:2.30048\n",
      "[12]\ttrain-mlogloss:2.24225\teval-mlogloss:2.29715\n",
      "[13]\ttrain-mlogloss:2.23697\teval-mlogloss:2.29424\n",
      "[14]\ttrain-mlogloss:2.23218\teval-mlogloss:2.29171\n",
      "[15]\ttrain-mlogloss:2.22781\teval-mlogloss:2.28949\n",
      "[16]\ttrain-mlogloss:2.22382\teval-mlogloss:2.28755\n",
      "[17]\ttrain-mlogloss:2.22016\teval-mlogloss:2.28585\n",
      "[18]\ttrain-mlogloss:2.2168\teval-mlogloss:2.28435\n",
      "[19]\ttrain-mlogloss:2.2137\teval-mlogloss:2.28304\n",
      "[20]\ttrain-mlogloss:2.21084\teval-mlogloss:2.28188\n",
      "[21]\ttrain-mlogloss:2.20819\teval-mlogloss:2.28087\n",
      "[22]\ttrain-mlogloss:2.20573\teval-mlogloss:2.27998\n",
      "[23]\ttrain-mlogloss:2.20345\teval-mlogloss:2.2792\n",
      "[24]\ttrain-mlogloss:2.20132\teval-mlogloss:2.27852\n",
      "[25]\ttrain-mlogloss:2.19934\teval-mlogloss:2.27792\n",
      "[26]\ttrain-mlogloss:2.19748\teval-mlogloss:2.2774\n",
      "[27]\ttrain-mlogloss:2.19575\teval-mlogloss:2.27694\n",
      "[28]\ttrain-mlogloss:2.19412\teval-mlogloss:2.27655\n",
      "[29]\ttrain-mlogloss:2.1926\teval-mlogloss:2.27621\n",
      "[30]\ttrain-mlogloss:2.19116\teval-mlogloss:2.27591\n",
      "[31]\ttrain-mlogloss:2.18981\teval-mlogloss:2.27566\n",
      "[32]\ttrain-mlogloss:2.18854\teval-mlogloss:2.27544\n",
      "[33]\ttrain-mlogloss:2.18734\teval-mlogloss:2.27527\n",
      "[34]\ttrain-mlogloss:2.18621\teval-mlogloss:2.27511\n",
      "[35]\ttrain-mlogloss:2.18514\teval-mlogloss:2.27499\n",
      "[36]\ttrain-mlogloss:2.18412\teval-mlogloss:2.27489\n",
      "[37]\ttrain-mlogloss:2.18316\teval-mlogloss:2.27481\n",
      "[38]\ttrain-mlogloss:2.18225\teval-mlogloss:2.27475\n",
      "[39]\ttrain-mlogloss:2.18139\teval-mlogloss:2.27471\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "#from sklearn.feature_extraction import FeatureHasher\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "#from sklearn.decomposition import TruncatedSVD, SparsePCA\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#from sklearn.metrics import log_loss\n",
    "\n",
    "# Create bag-of-apps in character string format\n",
    "# first by event\n",
    "# then merge to generate larger bags by device\n",
    "\n",
    "##################\n",
    "#   App Events\n",
    "##################\n",
    "#print(\"# Read App Events\")\n",
    "#app_ev = pd.read_csv(\"F:kaggle_data/talking_data_mobile/app_events.csv\", usecols=['event_id', 'app_id'])\n",
    "# remove duplicates(app_id)\n",
    "#app_ev = app_ev.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "#    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "\n",
    "#app_ev.to_pickle('F:/app_ev')\n",
    "app_ev = pd.read_pickle('F:/app_ev')\n",
    "##################\n",
    "#     Events\n",
    "##################\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(\"F:kaggle_data/talking_data_mobile/events.csv\", usecols=['event_id','device_id','timestamp'])\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_ev)\n",
    "\n",
    "del app_ev\n",
    "\n",
    "print(events.shape)\n",
    "events = events.dropna()\n",
    "print(events.shape)\n",
    "\n",
    "#extract time feature\n",
    "#events['hour'] = pd.to_datetime(events.timestamp).dt.hour\n",
    "\n",
    "#when the event happen in the events dataset\n",
    "#%matplotlib inline\n",
    "#events.hour.plot(kind='hist',bins=24)\n",
    "\n",
    "# 1 to 5 clock in the morning \n",
    "#events.loc[events.hour <= 5,'count'] = 1.0\n",
    "#events.loc[events.hour > 5,'count'] = 0.0\n",
    "\n",
    "# how many times a person play in the morning \n",
    "#counts = events.groupby('device_id')['count'].sum()\n",
    "#group them\n",
    "#ind1 = counts == 0\n",
    "#ind2 = (counts >0) & (counts <5)\n",
    "#ind3 = counts >4\n",
    "#counts[ind1] = '零次'\n",
    "#counts[ind2] = '少次'\n",
    "#counts[ind3] = '多次'\n",
    "\n",
    "#del ind1, ind2, ind3\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"F:kaggle_data/talking_data_mobile/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_train.csv\", usecols=['device_id','group'])\n",
    "\n",
    "test = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_test.csv\")\n",
    "                  \n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "#f4 = counts.reset_index()               # time group\n",
    "\n",
    "del Df\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "#f4.columns.values[1] = 'feature'\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3), axis=0, ignore_index=True)\n",
    "\n",
    "print(' not add time group FLS.shape:' , FLS.shape)\n",
    "\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "print('device_ids:', device_ids.shape)\n",
    "print('feature_cs:', feature_cs.shape)\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "print('not add time train_sp:', train_sp.shape)\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "\n",
    "print('after select time train_sp:', train_sp.shape)\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.07,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 40, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add is_active feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read App Events\n",
      "# Read Events\n",
      "(3252950, 5)\n",
      "after dropna: (1488096, 5)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\anaconda3-4.1.1\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    581\u001b[0m             result = expressions.evaluate(op, str_rep, x, y,\n\u001b[1;32m--> 582\u001b[1;33m                                           raise_on_error=True, **eval_kwargs)\n\u001b[0m\u001b[0;32m    583\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3-4.1.1\\lib\\site-packages\\pandas\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, op_str, a, b, raise_on_error, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m         return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\n\u001b[1;32m--> 209\u001b[1;33m                          **eval_kwargs)\n\u001b[0m\u001b[0;32m    210\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_on_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3-4.1.1\\lib\\site-packages\\pandas\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b, raise_on_error, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3-4.1.1\\lib\\site-packages\\pandas\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b, raise_on_error, **eval_kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-bfb55a389da0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mis_active\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_active\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unknow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[0mis_active\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_active\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mis_active\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_active'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_active\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_active\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'ge'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0mevents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"device_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"app_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3-4.1.1\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(left, right, name, na_op)\u001b[0m\n\u001b[0;32m    649\u001b[0m                 \u001b[0mlvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             return left._constructor(wrap_results(na_op(lvalues, rvalues)),\n\u001b[0m\u001b[0;32m    652\u001b[0m                                      \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m                                      dtype=dtype)\n",
      "\u001b[1;32mC:\\anaconda3-4.1.1\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    590\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                 raise TypeError(\"{typ} cannot perform the operation \"\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "# Based on yibo's R script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "#from sklearn.feature_extraction import FeatureHasher\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "#from sklearn.decomposition import TruncatedSVD, SparsePCA\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#from sklearn.metrics import log_loss\n",
    "\n",
    "# Create bag-of-apps in character string format\n",
    "# first by event\n",
    "# then merge to generate larger bags by device\n",
    "\n",
    "##################\n",
    "#   App Events\n",
    "##################\n",
    "print(\"# Read App Events\")\n",
    "app_ev = pd.read_csv(\"F:kaggle_data/talking_data_mobile/app_events.csv\", usecols=['event_id', 'app_id', 'is_active'])\n",
    "#generate is_active feature\n",
    "is_active = app_ev.groupby('event_id').sum().is_active\n",
    "\n",
    "#remove duplicates(app_id)\n",
    "app_ev = app_ev.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "\n",
    "#app_ev.to_pickle('F:/app_ev')\n",
    "\n",
    "##################\n",
    "#     Events\n",
    "##################\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(\"F:kaggle_data/talking_data_mobile/events.csv\", usecols=['event_id','device_id','timestamp'])\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_ev)\n",
    "events['is_active'] = events['event_id'].map(is_active)\n",
    "\n",
    "del app_ev\n",
    "del is_active\n",
    "events.to_pickle('F:/events')\n",
    "\n",
    "print(events.shape)\n",
    "events = events.dropna()\n",
    "print('after dropna:' , events.shape)\n",
    "\n",
    "#extract time feature\n",
    "events['hour'] = pd.to_datetime(events.timestamp).dt.hour\n",
    "\n",
    "#when the event happen in the events dataset\n",
    "#%matplotlib inline\n",
    "#events.hour.plot(kind='hist',bins=24)\n",
    "\n",
    "# 1 to 4 clock in the morning \n",
    "events.loc[events.hour <= 4,'count'] = 1.0\n",
    "events.loc[events.hour > 4,'count'] = 0.0\n",
    "\n",
    "# how many times a person play in the morning \n",
    "counts = events.groupby('device_id')['count'].sum()\n",
    "#group feature\n",
    "counts = counts.reset_index()\n",
    "counts['count'] = counts['count'].astype(np.str) + '次'\n",
    "#is_active feature\n",
    "is_active = events.groupby('device_id')['is_active'].sum()\n",
    "is_active = is_active.fillna('unknow')\n",
    "is_active = is_active.reset_index()\n",
    "is_active['is_active'] = is_active['is_active'].astype(np.str) + '个'\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"F:kaggle_data/talking_data_mobile/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_train.csv\", usecols=['device_id','group'])\n",
    "\n",
    "test = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_test.csv\")\n",
    "                  \n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "         # time group\n",
    "\n",
    "del Df\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "counts.columns.values[1] = 'feature'\n",
    "is_active.columns.values[1] = 'feature'\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, counts, is_active), axis=0, ignore_index=True)\n",
    "\n",
    "print('add time group FLS.shape:' , FLS.shape)\n",
    "\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "print('device_ids:', device_ids.shape)\n",
    "print('feature_cs:', feature_cs.shape)\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "print('add time train_sp:', train_sp.shape)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "print('after select time train_sp:', train_sp.shape)\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.05,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 35, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>is_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9222956879900151005</td>\n",
       "      <td>710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9222661944218806987</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9222399302879214035</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9221825537663503111</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9221767098072603291</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             device_id  is_active\n",
       "0 -9222956879900151005      710.0\n",
       "1 -9222661944218806987       46.0\n",
       "2 -9222399302879214035       20.0\n",
       "3 -9221825537663503111      252.0\n",
       "4 -9221767098072603291       79.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_active.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "add time group FLS.shape: (2864101, 2)\n",
      "# User-Item-Feature\n",
      "device_ids: (189076,)\n",
      "feature_cs: (23627,)\n",
      "add time train_sp: (74645, 23627)\n",
      "# Feature Selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3-4.1.1\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ..., 0 0 0] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after select time train_sp: (74645, 5434)\n",
      "# Num of Features:  5434\n",
      "[0]\ttrain-mlogloss:2.42399\teval-mlogloss:2.43209\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.39328\teval-mlogloss:2.40646\n",
      "[2]\ttrain-mlogloss:2.371\teval-mlogloss:2.3885\n",
      "[3]\ttrain-mlogloss:2.35317\teval-mlogloss:2.3745\n",
      "[4]\ttrain-mlogloss:2.33815\teval-mlogloss:2.363\n",
      "[5]\ttrain-mlogloss:2.32516\teval-mlogloss:2.35327\n",
      "[6]\ttrain-mlogloss:2.31374\teval-mlogloss:2.34491\n",
      "[7]\ttrain-mlogloss:2.30358\teval-mlogloss:2.33762\n",
      "[8]\ttrain-mlogloss:2.29447\teval-mlogloss:2.33123\n",
      "[9]\ttrain-mlogloss:2.28624\teval-mlogloss:2.32558\n",
      "[10]\ttrain-mlogloss:2.27877\teval-mlogloss:2.32056\n",
      "[11]\ttrain-mlogloss:2.27195\teval-mlogloss:2.31607\n",
      "[12]\ttrain-mlogloss:2.26571\teval-mlogloss:2.31207\n",
      "[13]\ttrain-mlogloss:2.25997\teval-mlogloss:2.30848\n",
      "[14]\ttrain-mlogloss:2.25468\teval-mlogloss:2.30524\n",
      "[15]\ttrain-mlogloss:2.24979\teval-mlogloss:2.30232\n",
      "[16]\ttrain-mlogloss:2.24525\teval-mlogloss:2.29968\n",
      "[17]\ttrain-mlogloss:2.24104\teval-mlogloss:2.29729\n",
      "[18]\ttrain-mlogloss:2.23712\teval-mlogloss:2.29512\n",
      "[19]\ttrain-mlogloss:2.23346\teval-mlogloss:2.29315\n",
      "[20]\ttrain-mlogloss:2.23004\teval-mlogloss:2.29136\n",
      "[21]\ttrain-mlogloss:2.22684\teval-mlogloss:2.28974\n",
      "[22]\ttrain-mlogloss:2.22384\teval-mlogloss:2.28825\n",
      "[23]\ttrain-mlogloss:2.22102\teval-mlogloss:2.2869\n",
      "[24]\ttrain-mlogloss:2.21837\teval-mlogloss:2.28567\n",
      "[25]\ttrain-mlogloss:2.21588\teval-mlogloss:2.28455\n",
      "[26]\ttrain-mlogloss:2.21352\teval-mlogloss:2.28353\n",
      "[27]\ttrain-mlogloss:2.2113\teval-mlogloss:2.28259\n",
      "[28]\ttrain-mlogloss:2.2092\teval-mlogloss:2.28174\n",
      "[29]\ttrain-mlogloss:2.20722\teval-mlogloss:2.28097\n",
      "[30]\ttrain-mlogloss:2.20534\teval-mlogloss:2.28025\n",
      "[31]\ttrain-mlogloss:2.20355\teval-mlogloss:2.27961\n",
      "[32]\ttrain-mlogloss:2.20186\teval-mlogloss:2.27902\n",
      "[33]\ttrain-mlogloss:2.20025\teval-mlogloss:2.27848\n",
      "[34]\ttrain-mlogloss:2.19872\teval-mlogloss:2.278\n"
     ]
    }
   ],
   "source": [
    "is_active['is_active'] = is_active['is_active'].astype(np.str) + '个'\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"F:kaggle_data/talking_data_mobile/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_train.csv\", usecols=['device_id','group'])\n",
    "\n",
    "test = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_test.csv\")\n",
    "                  \n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "         # time group\n",
    "\n",
    "del Df\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "counts.columns.values[1] = 'feature'\n",
    "is_active.columns.values[1] = 'feature'\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, counts, is_active), axis=0, ignore_index=True)\n",
    "\n",
    "print('add time group FLS.shape:' , FLS.shape)\n",
    "\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "print('device_ids:', device_ids.shape)\n",
    "print('feature_cs:', feature_cs.shape)\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "print('add time train_sp:', train_sp.shape)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "print('after select time train_sp:', train_sp.shape)\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.05,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 35, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.41068\teval-mlogloss:2.42089\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.37543\teval-mlogloss:2.39202\n",
      "[2]\ttrain-mlogloss:2.35017\teval-mlogloss:2.37218\n",
      "[3]\ttrain-mlogloss:2.33027\teval-mlogloss:2.35707\n",
      "[4]\ttrain-mlogloss:2.31382\teval-mlogloss:2.34498\n",
      "[5]\ttrain-mlogloss:2.29987\teval-mlogloss:2.33503\n",
      "[6]\ttrain-mlogloss:2.28784\teval-mlogloss:2.32672\n",
      "[7]\ttrain-mlogloss:2.27735\teval-mlogloss:2.31969\n",
      "[8]\ttrain-mlogloss:2.26811\teval-mlogloss:2.31368\n",
      "[9]\ttrain-mlogloss:2.25992\teval-mlogloss:2.30852\n",
      "[10]\ttrain-mlogloss:2.25261\teval-mlogloss:2.30408\n",
      "[11]\ttrain-mlogloss:2.24605\teval-mlogloss:2.30022\n",
      "[12]\ttrain-mlogloss:2.24014\teval-mlogloss:2.29687\n",
      "[13]\ttrain-mlogloss:2.23479\teval-mlogloss:2.29395\n",
      "[14]\ttrain-mlogloss:2.22993\teval-mlogloss:2.29139\n",
      "[15]\ttrain-mlogloss:2.2255\teval-mlogloss:2.28915\n",
      "[16]\ttrain-mlogloss:2.22145\teval-mlogloss:2.28719\n",
      "[17]\ttrain-mlogloss:2.21773\teval-mlogloss:2.28546\n",
      "[18]\ttrain-mlogloss:2.21432\teval-mlogloss:2.28395\n",
      "[19]\ttrain-mlogloss:2.21117\teval-mlogloss:2.28262\n",
      "[20]\ttrain-mlogloss:2.20826\teval-mlogloss:2.28145\n",
      "[21]\ttrain-mlogloss:2.20556\teval-mlogloss:2.28042\n",
      "[22]\ttrain-mlogloss:2.20307\teval-mlogloss:2.27952\n",
      "[23]\ttrain-mlogloss:2.20074\teval-mlogloss:2.27873\n",
      "[24]\ttrain-mlogloss:2.19858\teval-mlogloss:2.27803\n",
      "[25]\ttrain-mlogloss:2.19656\teval-mlogloss:2.27743\n",
      "[26]\ttrain-mlogloss:2.19467\teval-mlogloss:2.27689\n",
      "[27]\ttrain-mlogloss:2.19291\teval-mlogloss:2.27643\n",
      "[28]\ttrain-mlogloss:2.19125\teval-mlogloss:2.27603\n",
      "[29]\ttrain-mlogloss:2.1897\teval-mlogloss:2.27568\n",
      "[30]\ttrain-mlogloss:2.18824\teval-mlogloss:2.27538\n",
      "[31]\ttrain-mlogloss:2.18686\teval-mlogloss:2.27512\n",
      "[32]\ttrain-mlogloss:2.18557\teval-mlogloss:2.2749\n",
      "[33]\ttrain-mlogloss:2.18435\teval-mlogloss:2.27472\n",
      "[34]\ttrain-mlogloss:2.18319\teval-mlogloss:2.27456\n",
      "[35]\ttrain-mlogloss:2.1821\teval-mlogloss:2.27444\n",
      "[36]\ttrain-mlogloss:2.18107\teval-mlogloss:2.27433\n",
      "[37]\ttrain-mlogloss:2.18009\teval-mlogloss:2.27425\n",
      "[38]\ttrain-mlogloss:2.17916\teval-mlogloss:2.27419\n",
      "[39]\ttrain-mlogloss:2.17828\teval-mlogloss:2.27414\n",
      "[40]\ttrain-mlogloss:2.17745\teval-mlogloss:2.27412\n",
      "[41]\ttrain-mlogloss:2.17665\teval-mlogloss:2.2741\n",
      "[42]\ttrain-mlogloss:2.17589\teval-mlogloss:2.2741\n",
      "[43]\ttrain-mlogloss:2.17517\teval-mlogloss:2.27411\n",
      "[44]\ttrain-mlogloss:2.17449\teval-mlogloss:2.27412\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.07,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 45, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train\n"
     ]
    }
   ],
   "source": [
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 40, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('fine_tune.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9223321966609553846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9223067244542181226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9223042152723782980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9222956879900151005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9222896629442493034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-9222894989445037972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-9222894319703307262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-9222754701995937853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-9222661944218806987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-9222399302879214035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-9222352239947207574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-9222173362545970626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-9221825537663503111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-9221768839350705746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-9221767098072603291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-9221674814957667064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-9221639938103564513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-9221554258551357785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-9221307795397202665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-9221086586254644858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-9221079146476055829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-9221066489596332354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-9221046405740900422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-9221026417907250887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-9221015678978880842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-9220961720447724253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-9220830859283101130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-9220733369151052329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-9220727250496861488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-9220452176650064280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189046</th>\n",
       "      <td>9219686542557325817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189047</th>\n",
       "      <td>9219842210460037807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189048</th>\n",
       "      <td>9219926280825642237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189049</th>\n",
       "      <td>9219937375310355234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189050</th>\n",
       "      <td>9219958455132520777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189051</th>\n",
       "      <td>9220025918063413114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189052</th>\n",
       "      <td>9220160557900894171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189053</th>\n",
       "      <td>9220562120895859549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189054</th>\n",
       "      <td>9220807070557263555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189055</th>\n",
       "      <td>9220814716773471568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189056</th>\n",
       "      <td>9220880169487906579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189057</th>\n",
       "      <td>9220914901466458680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189058</th>\n",
       "      <td>9221114774124234731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189059</th>\n",
       "      <td>9221149157342105139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189060</th>\n",
       "      <td>9221152396628736959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189061</th>\n",
       "      <td>9221297143137682579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189062</th>\n",
       "      <td>9221586026451102237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189063</th>\n",
       "      <td>9221608286127666096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189064</th>\n",
       "      <td>9221693095468078153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189065</th>\n",
       "      <td>9221768426357971629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189066</th>\n",
       "      <td>9221843411551060582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189067</th>\n",
       "      <td>9222110179000857683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189068</th>\n",
       "      <td>9222172248989688166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189069</th>\n",
       "      <td>9222214407720961524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189070</th>\n",
       "      <td>9222355582733155698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189071</th>\n",
       "      <td>9222539910510672930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189072</th>\n",
       "      <td>9222779211060772275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189073</th>\n",
       "      <td>9222784289318287993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189074</th>\n",
       "      <td>9222849349208140841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189075</th>\n",
       "      <td>9223069070668353002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189076 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "0      -9223321966609553846\n",
       "1      -9223067244542181226\n",
       "2      -9223042152723782980\n",
       "3      -9222956879900151005\n",
       "4      -9222896629442493034\n",
       "5      -9222894989445037972\n",
       "6      -9222894319703307262\n",
       "7      -9222754701995937853\n",
       "8      -9222661944218806987\n",
       "9      -9222399302879214035\n",
       "10     -9222352239947207574\n",
       "11     -9222173362545970626\n",
       "12     -9221825537663503111\n",
       "13     -9221768839350705746\n",
       "14     -9221767098072603291\n",
       "15     -9221674814957667064\n",
       "16     -9221639938103564513\n",
       "17     -9221554258551357785\n",
       "18     -9221307795397202665\n",
       "19     -9221086586254644858\n",
       "20     -9221079146476055829\n",
       "21     -9221066489596332354\n",
       "22     -9221046405740900422\n",
       "23     -9221026417907250887\n",
       "24     -9221015678978880842\n",
       "25     -9220961720447724253\n",
       "26     -9220830859283101130\n",
       "27     -9220733369151052329\n",
       "28     -9220727250496861488\n",
       "29     -9220452176650064280\n",
       "...                     ...\n",
       "189046  9219686542557325817\n",
       "189047  9219842210460037807\n",
       "189048  9219926280825642237\n",
       "189049  9219937375310355234\n",
       "189050  9219958455132520777\n",
       "189051  9220025918063413114\n",
       "189052  9220160557900894171\n",
       "189053  9220562120895859549\n",
       "189054  9220807070557263555\n",
       "189055  9220814716773471568\n",
       "189056  9220880169487906579\n",
       "189057  9220914901466458680\n",
       "189058  9221114774124234731\n",
       "189059  9221149157342105139\n",
       "189060  9221152396628736959\n",
       "189061  9221297143137682579\n",
       "189062  9221586026451102237\n",
       "189063  9221608286127666096\n",
       "189064  9221693095468078153\n",
       "189065  9221768426357971629\n",
       "189066  9221843411551060582\n",
       "189067  9222110179000857683\n",
       "189068  9222172248989688166\n",
       "189069  9222214407720961524\n",
       "189070  9222355582733155698\n",
       "189071  9222539910510672930\n",
       "189072  9222779211060772275\n",
       "189073  9222784289318287993\n",
       "189074  9222849349208140841\n",
       "189075  9223069070668353002\n",
       "\n",
       "[189076 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2803279, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add time group\n",
    "FLS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74645, 4823)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.42441\teval-mlogloss:2.43213\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.39393\teval-mlogloss:2.40658\n",
      "[2]\ttrain-mlogloss:2.37182\teval-mlogloss:2.38865\n",
      "[3]\ttrain-mlogloss:2.3541\teval-mlogloss:2.37466\n",
      "[4]\ttrain-mlogloss:2.33917\teval-mlogloss:2.36314\n",
      "[5]\ttrain-mlogloss:2.32625\teval-mlogloss:2.3534\n",
      "[6]\ttrain-mlogloss:2.3149\teval-mlogloss:2.34503\n",
      "[7]\ttrain-mlogloss:2.30481\teval-mlogloss:2.33774\n",
      "[8]\ttrain-mlogloss:2.29576\teval-mlogloss:2.33134\n",
      "[9]\ttrain-mlogloss:2.28759\teval-mlogloss:2.32569\n",
      "[10]\ttrain-mlogloss:2.28017\teval-mlogloss:2.32066\n",
      "[11]\ttrain-mlogloss:2.27341\teval-mlogloss:2.31619\n",
      "[12]\ttrain-mlogloss:2.26722\teval-mlogloss:2.31218\n",
      "[13]\ttrain-mlogloss:2.26153\teval-mlogloss:2.30858\n",
      "[14]\ttrain-mlogloss:2.25629\teval-mlogloss:2.30534\n",
      "[15]\ttrain-mlogloss:2.25144\teval-mlogloss:2.30242\n",
      "[16]\ttrain-mlogloss:2.24695\teval-mlogloss:2.29978\n",
      "[17]\ttrain-mlogloss:2.24278\teval-mlogloss:2.29739\n",
      "[18]\ttrain-mlogloss:2.2389\teval-mlogloss:2.29522\n",
      "[19]\ttrain-mlogloss:2.23528\teval-mlogloss:2.29325\n",
      "[20]\ttrain-mlogloss:2.2319\teval-mlogloss:2.29147\n",
      "[21]\ttrain-mlogloss:2.22874\teval-mlogloss:2.28984\n",
      "[22]\ttrain-mlogloss:2.22577\teval-mlogloss:2.28836\n",
      "[23]\ttrain-mlogloss:2.22299\teval-mlogloss:2.28701\n",
      "[24]\ttrain-mlogloss:2.22037\teval-mlogloss:2.28577\n",
      "[25]\ttrain-mlogloss:2.21791\teval-mlogloss:2.28465\n",
      "[26]\ttrain-mlogloss:2.21558\teval-mlogloss:2.28363\n",
      "[27]\ttrain-mlogloss:2.21339\teval-mlogloss:2.28269\n",
      "[28]\ttrain-mlogloss:2.21132\teval-mlogloss:2.28184\n",
      "[29]\ttrain-mlogloss:2.20936\teval-mlogloss:2.28106\n",
      "[30]\ttrain-mlogloss:2.2075\teval-mlogloss:2.28036\n",
      "[31]\ttrain-mlogloss:2.20574\teval-mlogloss:2.27971\n",
      "[32]\ttrain-mlogloss:2.20407\teval-mlogloss:2.27912\n",
      "[33]\ttrain-mlogloss:2.20248\teval-mlogloss:2.27859\n",
      "[34]\ttrain-mlogloss:2.20098\teval-mlogloss:2.2781\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.05,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 35, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train\n"
     ]
    }
   ],
   "source": [
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 30, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('fine_tune.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74645, 4823)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20970"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLS.feature.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter twick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read App Events\n",
      "# Read Events\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "# User-Item-Feature\n",
      "# Feature Selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3-4.1.1\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ..., 0 0 0] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Num of Features:  4823\n",
      "[0]\ttrain-mlogloss:2.418\teval-mlogloss:2.42501\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.38685\teval-mlogloss:2.39787\n",
      "[2]\ttrain-mlogloss:2.36486\teval-mlogloss:2.37919\n",
      "[3]\ttrain-mlogloss:2.34767\teval-mlogloss:2.36487\n",
      "[4]\ttrain-mlogloss:2.33354\teval-mlogloss:2.35331\n",
      "[5]\ttrain-mlogloss:2.32161\teval-mlogloss:2.34371\n",
      "[6]\ttrain-mlogloss:2.31136\teval-mlogloss:2.33561\n",
      "[7]\ttrain-mlogloss:2.30246\teval-mlogloss:2.32868\n",
      "[8]\ttrain-mlogloss:2.29465\teval-mlogloss:2.32272\n",
      "[9]\ttrain-mlogloss:2.28775\teval-mlogloss:2.31755\n",
      "[10]\ttrain-mlogloss:2.28163\teval-mlogloss:2.31304\n",
      "[11]\ttrain-mlogloss:2.27616\teval-mlogloss:2.30909\n",
      "[12]\ttrain-mlogloss:2.27125\teval-mlogloss:2.30562\n",
      "[13]\ttrain-mlogloss:2.26683\teval-mlogloss:2.30255\n",
      "[14]\ttrain-mlogloss:2.26283\teval-mlogloss:2.29984\n",
      "[15]\ttrain-mlogloss:2.2592\teval-mlogloss:2.29743\n",
      "[16]\ttrain-mlogloss:2.25589\teval-mlogloss:2.29529\n",
      "[17]\ttrain-mlogloss:2.25288\teval-mlogloss:2.29338\n",
      "[18]\ttrain-mlogloss:2.25012\teval-mlogloss:2.29167\n",
      "[19]\ttrain-mlogloss:2.24758\teval-mlogloss:2.29014\n",
      "[20]\ttrain-mlogloss:2.24525\teval-mlogloss:2.28877\n",
      "[21]\ttrain-mlogloss:2.2431\teval-mlogloss:2.28753\n",
      "[22]\ttrain-mlogloss:2.24112\teval-mlogloss:2.28642\n",
      "[23]\ttrain-mlogloss:2.23928\teval-mlogloss:2.28542\n",
      "[24]\ttrain-mlogloss:2.23758\teval-mlogloss:2.28451\n",
      "[25]\ttrain-mlogloss:2.236\teval-mlogloss:2.28369\n",
      "[26]\ttrain-mlogloss:2.23452\teval-mlogloss:2.28295\n",
      "[27]\ttrain-mlogloss:2.23315\teval-mlogloss:2.28228\n",
      "[28]\ttrain-mlogloss:2.23187\teval-mlogloss:2.28167\n",
      "[29]\ttrain-mlogloss:2.23067\teval-mlogloss:2.28112\n",
      "[30]\ttrain-mlogloss:2.22955\teval-mlogloss:2.28062\n",
      "[31]\ttrain-mlogloss:2.2285\teval-mlogloss:2.28017\n",
      "[32]\ttrain-mlogloss:2.22751\teval-mlogloss:2.27976\n",
      "[33]\ttrain-mlogloss:2.22658\teval-mlogloss:2.27939\n",
      "[34]\ttrain-mlogloss:2.22571\teval-mlogloss:2.27905\n",
      "[35]\ttrain-mlogloss:2.22489\teval-mlogloss:2.27874\n",
      "[36]\ttrain-mlogloss:2.22411\teval-mlogloss:2.27846\n",
      "[37]\ttrain-mlogloss:2.22338\teval-mlogloss:2.2782\n",
      "[38]\ttrain-mlogloss:2.22269\teval-mlogloss:2.27797\n",
      "[39]\ttrain-mlogloss:2.22204\teval-mlogloss:2.27776\n"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "# Based on yibo's R script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "#from sklearn.feature_extraction import FeatureHasher\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "#from sklearn.decomposition import TruncatedSVD, SparsePCA\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#from sklearn.metrics import log_loss\n",
    "\n",
    "# Create bag-of-apps in character string format\n",
    "# first by event\n",
    "# then merge to generate larger bags by device\n",
    "\n",
    "##################\n",
    "#   App Events\n",
    "##################\n",
    "print(\"# Read App Events\")\n",
    "app_ev = pd.read_csv(\"F:kaggle_data/talking_data_mobile/app_events.csv\", usecols=['event_id', 'app_id'])\n",
    "# remove duplicates(app_id)\n",
    "app_ev = app_ev.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "\n",
    "##################\n",
    "#     Events\n",
    "##################\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(\"F:kaggle_data/talking_data_mobile/events.csv\", usecols=['event_id','device_id'])\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_ev)\n",
    "\n",
    "events = events.dropna()\n",
    "\n",
    "del app_ev\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"F:kaggle_data/talking_data_mobile/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_train.csv\", usecols=['device_id','group'])\n",
    "\n",
    "test = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_test.csv\")\n",
    "                  \n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "\n",
    "del Df\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3), axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 3,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.07,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 5,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 40, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.43232\teval-mlogloss:2.43422\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:2.40458\teval-mlogloss:2.40708\n",
      "[2]\ttrain-mlogloss:2.38432\teval-mlogloss:2.38716\n",
      "[3]\ttrain-mlogloss:2.36798\teval-mlogloss:2.37104\n",
      "[4]\ttrain-mlogloss:2.35414\teval-mlogloss:2.35734\n",
      "[5]\ttrain-mlogloss:2.34206\teval-mlogloss:2.34538\n",
      "[6]\ttrain-mlogloss:2.33136\teval-mlogloss:2.33478\n",
      "[7]\ttrain-mlogloss:2.32175\teval-mlogloss:2.32526\n",
      "[8]\ttrain-mlogloss:2.31306\teval-mlogloss:2.31666\n",
      "[9]\ttrain-mlogloss:2.30514\teval-mlogloss:2.30882\n",
      "[10]\ttrain-mlogloss:2.29789\teval-mlogloss:2.30166\n",
      "[11]\ttrain-mlogloss:2.29122\teval-mlogloss:2.29508\n",
      "[12]\ttrain-mlogloss:2.28506\teval-mlogloss:2.289\n",
      "[13]\ttrain-mlogloss:2.27935\teval-mlogloss:2.28338\n",
      "[14]\ttrain-mlogloss:2.27405\teval-mlogloss:2.27817\n",
      "[15]\ttrain-mlogloss:2.26912\teval-mlogloss:2.27332\n",
      "[16]\ttrain-mlogloss:2.26451\teval-mlogloss:2.2688\n",
      "[17]\ttrain-mlogloss:2.2602\teval-mlogloss:2.26457\n",
      "[18]\ttrain-mlogloss:2.25616\teval-mlogloss:2.26061\n",
      "[19]\ttrain-mlogloss:2.25237\teval-mlogloss:2.2569\n",
      "[20]\ttrain-mlogloss:2.2488\teval-mlogloss:2.25341\n",
      "[21]\ttrain-mlogloss:2.24544\teval-mlogloss:2.25012\n",
      "[22]\ttrain-mlogloss:2.24227\teval-mlogloss:2.24702\n",
      "[23]\ttrain-mlogloss:2.23928\teval-mlogloss:2.2441\n",
      "[24]\ttrain-mlogloss:2.23645\teval-mlogloss:2.24134\n",
      "[25]\ttrain-mlogloss:2.23377\teval-mlogloss:2.23872\n",
      "[26]\ttrain-mlogloss:2.23123\teval-mlogloss:2.23624\n",
      "[27]\ttrain-mlogloss:2.22882\teval-mlogloss:2.23389\n",
      "[28]\ttrain-mlogloss:2.22654\teval-mlogloss:2.23166\n",
      "[29]\ttrain-mlogloss:2.22436\teval-mlogloss:2.22954\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 7,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.04,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 30, evals=watchlist,\n",
    "                early_stopping_rounds=10, verbose_eval=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train\n"
     ]
    }
   ],
   "source": [
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 30, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('sub17.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Documents\\\\python_file\\\\kaggle\\\\Talking Data Mobile'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.41799\teval-mlogloss:2.42502\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.38687\teval-mlogloss:2.39791\n",
      "[2]\ttrain-mlogloss:2.36487\teval-mlogloss:2.37922\n",
      "[3]\ttrain-mlogloss:2.34768\teval-mlogloss:2.3649\n",
      "[4]\ttrain-mlogloss:2.33355\teval-mlogloss:2.35334\n",
      "[5]\ttrain-mlogloss:2.32161\teval-mlogloss:2.34374\n",
      "[6]\ttrain-mlogloss:2.31136\teval-mlogloss:2.33563\n",
      "[7]\ttrain-mlogloss:2.30246\teval-mlogloss:2.32871\n",
      "[8]\ttrain-mlogloss:2.29465\teval-mlogloss:2.32274\n",
      "[9]\ttrain-mlogloss:2.28776\teval-mlogloss:2.31757\n",
      "[10]\ttrain-mlogloss:2.28163\teval-mlogloss:2.31306\n",
      "[11]\ttrain-mlogloss:2.27616\teval-mlogloss:2.30911\n",
      "[12]\ttrain-mlogloss:2.27125\teval-mlogloss:2.30565\n",
      "[13]\ttrain-mlogloss:2.26683\teval-mlogloss:2.30258\n",
      "[14]\ttrain-mlogloss:2.26283\teval-mlogloss:2.29987\n",
      "[15]\ttrain-mlogloss:2.2592\teval-mlogloss:2.29746\n",
      "[16]\ttrain-mlogloss:2.2559\teval-mlogloss:2.29532\n",
      "[17]\ttrain-mlogloss:2.25288\teval-mlogloss:2.2934\n",
      "[18]\ttrain-mlogloss:2.25012\teval-mlogloss:2.29169\n",
      "[19]\ttrain-mlogloss:2.24759\teval-mlogloss:2.29016\n",
      "[20]\ttrain-mlogloss:2.24526\teval-mlogloss:2.28879\n",
      "[21]\ttrain-mlogloss:2.24311\teval-mlogloss:2.28755\n",
      "[22]\ttrain-mlogloss:2.24112\teval-mlogloss:2.28644\n",
      "[23]\ttrain-mlogloss:2.23929\teval-mlogloss:2.28544\n",
      "[24]\ttrain-mlogloss:2.23758\teval-mlogloss:2.28453\n",
      "[25]\ttrain-mlogloss:2.236\teval-mlogloss:2.28371\n",
      "[26]\ttrain-mlogloss:2.23452\teval-mlogloss:2.28297\n",
      "[27]\ttrain-mlogloss:2.23315\teval-mlogloss:2.2823\n",
      "[28]\ttrain-mlogloss:2.23187\teval-mlogloss:2.28169\n",
      "[29]\ttrain-mlogloss:2.23067\teval-mlogloss:2.28114\n",
      "# Train\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 3,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.07,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 5,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 30, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)\n",
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 20, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('sub21.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add hour（每个events的hour） feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read App Events\n",
      "# Read Events\n",
      "# Read Phone Brand\n",
      "# Generate Train and Test\n",
      "# User-Item-Feature\n",
      "# Feature Selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3-4.1.1\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ..., 0 0 0] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Num of Features:  4828\n",
      "[0]\ttrain-mlogloss:2.41089\teval-mlogloss:2.4205\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:2.37573\teval-mlogloss:2.39159\n",
      "[2]\ttrain-mlogloss:2.35057\teval-mlogloss:2.37176\n",
      "[3]\ttrain-mlogloss:2.33073\teval-mlogloss:2.35667\n",
      "[4]\ttrain-mlogloss:2.31434\teval-mlogloss:2.34457\n",
      "[5]\ttrain-mlogloss:2.30046\teval-mlogloss:2.33463\n",
      "[6]\ttrain-mlogloss:2.2885\teval-mlogloss:2.32631\n",
      "[7]\ttrain-mlogloss:2.27808\teval-mlogloss:2.31928\n",
      "[8]\ttrain-mlogloss:2.26891\teval-mlogloss:2.31329\n",
      "[9]\ttrain-mlogloss:2.26079\teval-mlogloss:2.30815\n",
      "[10]\ttrain-mlogloss:2.25354\teval-mlogloss:2.30372\n",
      "[11]\ttrain-mlogloss:2.24704\teval-mlogloss:2.29987\n",
      "[12]\ttrain-mlogloss:2.24118\teval-mlogloss:2.29654\n",
      "[13]\ttrain-mlogloss:2.23588\teval-mlogloss:2.29363\n",
      "[14]\ttrain-mlogloss:2.23107\teval-mlogloss:2.29109\n",
      "[15]\ttrain-mlogloss:2.22668\teval-mlogloss:2.28887\n",
      "[16]\ttrain-mlogloss:2.22267\teval-mlogloss:2.28692\n",
      "[17]\ttrain-mlogloss:2.21899\teval-mlogloss:2.28522\n",
      "[18]\ttrain-mlogloss:2.21561\teval-mlogloss:2.28371\n",
      "[19]\ttrain-mlogloss:2.21249\teval-mlogloss:2.2824\n",
      "[20]\ttrain-mlogloss:2.20961\teval-mlogloss:2.28124\n",
      "[21]\ttrain-mlogloss:2.20694\teval-mlogloss:2.28023\n",
      "[22]\ttrain-mlogloss:2.20447\teval-mlogloss:2.27933\n",
      "[23]\ttrain-mlogloss:2.20217\teval-mlogloss:2.27855\n",
      "[24]\ttrain-mlogloss:2.20002\teval-mlogloss:2.27786\n",
      "[25]\ttrain-mlogloss:2.19802\teval-mlogloss:2.27726\n",
      "[26]\ttrain-mlogloss:2.19615\teval-mlogloss:2.27674\n",
      "[27]\ttrain-mlogloss:2.1944\teval-mlogloss:2.27628\n",
      "[28]\ttrain-mlogloss:2.19276\teval-mlogloss:2.27588\n",
      "[29]\ttrain-mlogloss:2.19122\teval-mlogloss:2.27554\n",
      "[30]\ttrain-mlogloss:2.18977\teval-mlogloss:2.27524\n",
      "[31]\ttrain-mlogloss:2.18841\teval-mlogloss:2.27499\n",
      "[32]\ttrain-mlogloss:2.18712\teval-mlogloss:2.27477\n",
      "[33]\ttrain-mlogloss:2.18591\teval-mlogloss:2.27459\n",
      "[34]\ttrain-mlogloss:2.18476\teval-mlogloss:2.27444\n",
      "[35]\ttrain-mlogloss:2.18368\teval-mlogloss:2.27432\n",
      "[36]\ttrain-mlogloss:2.18265\teval-mlogloss:2.27422\n",
      "[37]\ttrain-mlogloss:2.18168\teval-mlogloss:2.27414\n",
      "[38]\ttrain-mlogloss:2.18076\teval-mlogloss:2.27408\n",
      "[39]\ttrain-mlogloss:2.17988\teval-mlogloss:2.27404\n",
      "# Train\n"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "# Based on yibo's R script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "#from sklearn.feature_extraction import FeatureHasher\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "#from sklearn.decomposition import TruncatedSVD, SparsePCA\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#from sklearn.metrics import log_loss\n",
    "\n",
    "# Create bag-of-apps in character string format\n",
    "# first by event\n",
    "# then merge to generate larger bags by device\n",
    "\n",
    "##################\n",
    "#   App Events\n",
    "##################\n",
    "print(\"# Read App Events\")\n",
    "app_ev = pd.read_csv(\"F:kaggle_data/talking_data_mobile/app_events.csv\", usecols=['event_id', 'app_id'])\n",
    "# remove duplicates(app_id)\n",
    "app_ev = app_ev.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "\n",
    "##################\n",
    "#     Events\n",
    "##################\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(\"F:kaggle_data/talking_data_mobile/events.csv\", usecols=['event_id','device_id'])\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_ev)\n",
    "\n",
    "events = events.dropna()\n",
    "\n",
    "del app_ev\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"F:kaggle_data/talking_data_mobile/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_train.csv\", usecols=['device_id','group'])\n",
    "\n",
    "test = pd.read_csv(\"F:kaggle_data/talking_data_mobile/gender_age_test.csv\")\n",
    "                  \n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "\n",
    "f4 = pd.read_csv(\"F:/kaggle_data/talking_data_mobile/events.csv\", usecols=['device_id','timestamp'])\n",
    "f4['hour'] = pd.to_datetime(f4.timestamp).dt.hour\n",
    "f4.hour = f4.hour.astype(np.str) + 'h'\n",
    "f4.drop('timestamp',axis=1, inplace=True)\n",
    "\n",
    "del Df\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "f4.columns.values[1] = \"feature\"\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, f4), axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=.90, random_state=10)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "selector = SelectPercentile(f_classif, percentile=23)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_val = selector.transform(X_val)\n",
    "\n",
    "train_sp = selector.transform(train_sp)\n",
    "test_sp = selector.transform(test_sp)\n",
    "\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.07,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 3,\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 40, evals=watchlist,\n",
    "                early_stopping_rounds=25, verbose_eval=True)\n",
    "\n",
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 40, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('fine_tune.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<67180x4828 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1053878 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74645,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 9, ..., 0, 6, 9], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67180,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Documents\\\\python_file\\\\kaggle\\\\Talking Data Mobile'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.41716\teval-mlogloss:2.41884\n",
      "[1]\ttrain-mlogloss:2.38562\teval-mlogloss:2.38765\n",
      "[2]\ttrain-mlogloss:2.36335\teval-mlogloss:2.36553\n",
      "[3]\ttrain-mlogloss:2.34597\teval-mlogloss:2.34821\n",
      "[4]\ttrain-mlogloss:2.3317\teval-mlogloss:2.33394\n",
      "[5]\ttrain-mlogloss:2.31967\teval-mlogloss:2.32188\n",
      "[6]\ttrain-mlogloss:2.30936\teval-mlogloss:2.31155\n",
      "[7]\ttrain-mlogloss:2.30042\teval-mlogloss:2.30259\n",
      "[8]\ttrain-mlogloss:2.29258\teval-mlogloss:2.29475\n",
      "[9]\ttrain-mlogloss:2.28566\teval-mlogloss:2.28784\n",
      "[10]\ttrain-mlogloss:2.27952\teval-mlogloss:2.2817\n",
      "[11]\ttrain-mlogloss:2.27403\teval-mlogloss:2.27623\n",
      "[12]\ttrain-mlogloss:2.26911\teval-mlogloss:2.27132\n",
      "[13]\ttrain-mlogloss:2.26468\teval-mlogloss:2.26691\n",
      "[14]\ttrain-mlogloss:2.26067\teval-mlogloss:2.26291\n",
      "[15]\ttrain-mlogloss:2.25703\teval-mlogloss:2.25929\n",
      "[16]\ttrain-mlogloss:2.25372\teval-mlogloss:2.25599\n",
      "[17]\ttrain-mlogloss:2.2507\teval-mlogloss:2.25298\n",
      "[18]\ttrain-mlogloss:2.24793\teval-mlogloss:2.25022\n",
      "[19]\ttrain-mlogloss:2.24539\teval-mlogloss:2.24769\n",
      "[20]\ttrain-mlogloss:2.24305\teval-mlogloss:2.24536\n",
      "[21]\ttrain-mlogloss:2.2409\teval-mlogloss:2.24322\n",
      "[22]\ttrain-mlogloss:2.23891\teval-mlogloss:2.24124\n",
      "[23]\ttrain-mlogloss:2.23706\teval-mlogloss:2.2394\n",
      "[24]\ttrain-mlogloss:2.23536\teval-mlogloss:2.23769\n",
      "# Train\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 12,\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"max_depth\": 5,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.07,\n",
    "    \"silent\": 1,\n",
    "    \"alpha\": 5,\n",
    "    \n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, 25, evals=watchlist,\n",
    "                 verbose_eval=True)\n",
    "\n",
    "print(\"# Train\")\n",
    "dtrain = xgb.DMatrix(train_sp, Y)\n",
    "gbm = xgb.train(params, dtrain, 25, verbose_eval=True)\n",
    "y_pre = gbm.predict(xgb.DMatrix(test_sp))\n",
    "\n",
    "# Write results\n",
    "result = pd.DataFrame(y_pre, columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('fine_tune.gz', index=True,\n",
    "              index_label='device_id', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.38169928051\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_depth=8)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict_proba(X_val)\n",
    "print(log_loss(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.38169928051\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "pred = rf.predict_proba(X_val)\n",
    "print(log_loss(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.40303731, -2.39014326, -2.38879333, -2.38854669, -2.37988848,\n",
       "       -2.37984983, -2.37470023, -2.36904485, -2.35956989, -2.36430039])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, train_sp, Y, cv=10,scoring='log_loss')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
